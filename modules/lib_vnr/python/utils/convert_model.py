import argparse
import os.path
import subprocess
import tensorflow as tf
import numpy as np
import shutil
from xmos_ai_tools import xcore_tflm_host_interpreter as xtflm
import pkg_resources
import tempfile

this_filepath = os.path.dirname(os.path.abspath(__file__))

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument("tflite_model", nargs='?',
                        help="tflite model to convert and integrate into the Avona VNR module")
    parser.add_argument("--copy-files", action='store_true', help="Copy generated files to vnr module")
    parser.add_argument("--module-path", type=str, default=None, help="Path to vnr module to copy the new files to. Used when --copy-files")
    args = parser.parse_args()
    return args

def get_quant_spec(model):
    interpreter_tflite = tf.lite.Interpreter(model_path=str(model))
    input_details = interpreter_tflite.get_input_details()[0]
    output_details = interpreter_tflite.get_output_details()[0]    

    assert(input_details["dtype"] in [np.int8, np.uint8]),"Error: Only 8bit input supported"
    assert(output_details["dtype"] in [np.int8, np.uint8]),"Error: Only 8bit output supported"

    # quantization spec
    input_scale, input_zero_point = input_details["quantization"]
    output_scale, output_zero_point = output_details["quantization"]
    return input_scale, input_zero_point, output_scale, output_zero_point

if __name__ == "__main__":
    args = parse_arguments()
    print("copy_files = ",args.copy_files)
    model = args.tflite_model
    model = os.path.abspath(model)
    ai_tools_version = pkg_resources.get_distribution('xmos_ai_tools').version
    print(f"model file = {model}. Using xmos-ai-tools version {ai_tools_version}")

    tfilte_model_dir = os.path.dirname(os.path.abspath(model))

    test_dir = tempfile.mkdtemp(prefix=f"convert_{os.path.basename(model).split('.')[0]}_", dir=".")
    print(f"Generating output files to {test_dir} directory")

    # Tflite to xcore optimised tflite
    xcore_opt_model = os.path.join(test_dir, os.path.basename(model).split('.')[0] + "_xcore.tflite")
    convert_cmd = f"xcore-opt --xcore-thread-count 1 -o {xcore_opt_model} {model}".split()
    subprocess.run(convert_cmd, check=True)
    xcore_opt_model_size = os.path.getsize(xcore_opt_model)
    
    # Convert tflite to .c and .h files
    model_c_file = os.path.join(test_dir, "vnr_model_data.c")
    model_h_file = os.path.join(test_dir, "vnr_model_data.h")
    tflite_to_c_script = os.path.join(this_filepath, "convert_tflite_to_c_source.py")
    cmd = f"{tflite_to_c_script} --input {xcore_opt_model} --header {model_h_file} --source {model_c_file} --variable-name vnr".split()
    subprocess.run(cmd, check=True)
    
    # Double check model size against vnr_model_data_len printed in the vnr_model_data.c
    with open(model_c_file, "r") as fp:
        lines = fp.read().splitlines()
        for l in lines:
            if "vnr_model_data_len" in l:
                model_data_len = (l.split()[-1])[:-1] #TODO use re
                assert(xcore_opt_model_size == int(model_data_len)), "model_data_len doesn't match xcore_opt tflite file size"

    # Tensor arena size define file
    ie = xtflm.XTFLMInterpreter(model_path=xcore_opt_model)
    ie.allocate_tensors()
    print(f"Tensor arena size = {ie.tensor_arena_size} bytes")
    with open(os.path.join(test_dir, "vnr_tensor_arena_size.h"), "w") as fp:
        fp.write(f"// Autogenerated from {os.path.abspath(__file__)}. Do not modify\n")
        fp.write(f"// Generated using xmos-ai-tools version {ai_tools_version}\n")
        fp.write("#ifndef VNR_TENSOR_ARENA_SIZE_H\n")
        fp.write("#define VNR_TENSOR_ARENA_SIZE_H\n\n")
        fp.write(f"#define TENSOR_ARENA_SIZE_BYTES    ({ie.tensor_arena_size} - {xcore_opt_model_size}) // Remove model size from the tensor_arena_size returned by the python xtflm host interpreter \n")
        fp.write("\n#endif")
    
    # Quant dequant spec defines file
    input_scale, input_zero_point, output_scale, output_zero_point = get_quant_spec(model)
    print(f"input_scale {input_scale}, input_zero_point {input_zero_point}, output_scale {output_scale}, output_zero_point {output_zero_point}")
    with open(os.path.join(test_dir, "vnr_quant_spec_defines.h"), "w") as fp:
        fp.write(f"// Autogenerated from {os.path.abspath(__file__)}. Do not modify\n")
        fp.write(f"// Generated using xmos-ai-tools version {ai_tools_version}\n")
        fp.write("#ifndef VNR_QUANT_SPEC_DEFINES_H\n")
        fp.write("#define VNR_QUANT_SPEC_DEFINES_H\n\n")
        fp.write(f"#define VNR_INPUT_SCALE_INV    (1.0/{input_scale})\n")
        fp.write(f"#define VNR_INPUT_ZERO_POINT   ({input_zero_point})\n")
        fp.write(f"#define VNR_OUTPUT_SCALE       ({output_scale})\n")
        fp.write(f"#define VNR_OUTPUT_ZERO_POINT   ({output_zero_point})\n")
        fp.write("\n#endif")
    
    # Copy generated files into the VNR module
    if args.copy_files:
        assert(args.module_path != None), "VNR module path --module-path needs to be specified when running with --copy-files"
        vnr_module_path = os.path.abspath(args.module_path)
        print(f"WARNING: Copying files to {vnr_module_path} and {tflite_model_dir}. Verify before committing!")
        # Copy converted model .c and .h files
        shutil.copy2(model_c_file, os.path.join(vnr_module_path, "src/inference/model/"))
        shutil.copy2(model_h_file, os.path.join(vnr_module_path, "src/inference/model/"))
        # Copy tensor arena size defines file
        shutil.copy2(os.path.join(test_dir, "vnr_tensor_arena_size.h"), os.path.join(vnr_module_path, "api/inference/"))
        # Copy quant dequant spec defines file
        shutil.copy2(os.path.join(test_dir, "vnr_quant_spec_defines.h"), os.path.join(vnr_module_path, "api/inference/"))
        # Copy xcore opt model tflite file to the model's directory
        shutil.copy2(xcore_opt_model, tflite_model_dir)
        



            

